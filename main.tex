\documentclass[12pt, letterpaper, oneside]{article}
\usepackage[utf8]{inputenc}
\usepackage[LGR,T1]{fontenc}
\usepackage{braket}
\usepackage{amsmath}
\usepackage{biblatex}
\usepackage{bbold}

\addbibresource{refx.bib}

\title{Kvantno masinsko ucenje}
\author{Milan Bojic}
\date{Jun 2022}

\renewcommand*\contentsname{Sadrzaj}

\begin{document}

\maketitle
\newpage

\tableofcontents
\newpage
\section{Kvantno racunarstvo}
Pre nego sto se pocne pricati o Kvantnom masinskom ucenju, treba objasniti neki osnovni pojmovi da bi lakse razumeli ostatak rada

\subsection{Osnovni pojmovi}
Potrebni pojmovi su:
\begin{itemize}
    \item Kubit (eng. Qubit)
    \item Kvantna kola (eng. Quantum Gates)
    \item Kvantna uvezanost (eng. Quantum entanglement)
    \item Kvantan memorija, Kvantni registri
\end{itemize}

\subsubsection*{Kubit}
Kubit (eng. Qubit ) je najmanja jedinica informacije u kvantnom računarstvu, slično bit-u u klasičnom računarstvu.
Razlika od bita jeste u tome što kubit pored stanja 1 i 0, može da se nalazi i u superpoziciji između oba.
Oni se mogu predstaviti formulom (koristeci "bra-ket" notaciju):
\[ \ket{\gamma} =  \alpha\ket{0}+\beta\ket{1} \]
Ovde su $\ket{0}$ i $\ket{1}$ zapravo stanja kao i kod klasičnog bita, a $\alpha$ i $\beta$ su kompleksni brojevi koji predstavljaju aplitude zadatih stanja i za njih važi:

\[ |\alpha|^2+|\beta|^2 = 1 \]
Posto stanje kubita ima dva stepena slobode sto dovodi do toga da amplitude se mogu zapisati kao:
\[
    \alpha = \cos{\frac{\Theta}{2}} 
\]
\[
    \beta = e^{i\phi}\sin{\frac{\Theta}{2}}
\]
Takodje mozemo da vidimo da je $|\alpha|^2$ verovatnoca da se kubit nalazi u stanju 0, isto vazi i za $ |\beta|^2$ i 1.
Saznanje o tomo u kom stanju se nalazi kubit ce se dobiti merenjem kubita, tade ce da kubit izadje iz superpozicije i "pasce" u stanje 1 ili stanje 0. U tom slucaku kubit ce imati ponasanje kao i obican bit, ali ovako gubimo predjasnje kvantno stanje kubita.
U fizičkom svetu kubit se moze predstaviti kao polarizovani fotoni, pre cemu se dva stanje se uzimaju kao vertikalna i horizontalna polarizacija.
\subsubsection*{Kvantna kapija}
Kvantna kapije (eng. Quantum Gates ) su logički predstavljaju matricama i oni rade na određenom broju kubita.Matrice su unitarne sa oblikom $2^n \times 2^n$, gde je $n$ broj qubita na kojim radimo . Neke od poznatih kola su: Hademardovo kolo (stavalja kubit u superpoziciju), bit flip kolo (zamenjuje aplitude na kubitu), ali nas najviše zanima rotaciono kolo:
\[
    R = \begin{bmatrix}
        \cos{\Theta} & -\sin{\Theta} \\
        \sin{\Theta} & \cos{\Theta} 
    \end{bmatrix}
\]
Ovo kolo rotira kubite u prostoru, odnosno menja njihove amplitude za $\Theta$ radiana.
%% Pogledati Quantum Computation and Quantum Information ch. 1.3 Quantum computation za dopune
\subsubsection*{Kvantna uvezanost}
Kvantna uvezanost (eng. Quantum entanglement) je fizički pojam gde su dva ,ili više, kubita povezana tako da zajedno prave novo kvantno stanje.
U čistim stanjima oni su matematički zapravo proizvodi tenzora amplituda:
\[
    \ket{\gamma} \otimes \ket{\delta} = \alpha_1\alpha_2\ket{00} + \alpha_1\beta_2\ket{01} + \beta_1\alpha_2\ket{10} + \beta_1\beta_2\ket{11}
\]
I ovako napisano kvantno stanje se moze razdvojiti na dva kubita. Ali postoje i kvanta stanja koja se ne mogu razdvojiti npr.
\[
 \frac{1}{\sqrt{2}}\ket{00} + \frac{1}{\sqrt{2}}\ket{11}
\]
Zanimljiva stvar kod uvazanih kubita jeste u tome što dele informacije. Ako bi jedan kubit iz para odneli u neko veoma daleko mesto (na primer druga galaksija), i tamo bi ga izmerili mi bi smo dobili 0 ili 1, međutim drugi kubit bi takođe upao u određeno stanje i to u istom trenutni kad smo izmerili prvi daleki kubit. Ovo je zapravno gde se nalazi glavan različitost između klasičnog i kvantnog računarstva, ova pojava ne postoji u klasičnom računarstvu i ne može se "lako" simulirati.
\subsubsection*{Kvantni registri}
Kvantni registri se sastoje od kvantnog stanja od $m$ uvezanih kubita i moze da se predstavlja do $2^m$ vrednosti stanja istovremeno.
Kvantan memorija su uredjaji koji cuvaju kvantna stanja fotona, bez da unistavaju kvanten informacije koja se nalazi u fotonu.
Ovakva memorija zahteva koherentni sistem materije, jer bi u suprotnom kvantna informacija unitar uredjaja bila izgubljena zbog nekoherentnosti.

\subsection{Kvantno racunarstvo}
Kvantno racunarstvo je vrsta racunarstva gde se koriste kolekcije fizickih osobina kvantne mehanike kao sto su superpozicija i kvantna uvezanost,
tako da se izvrsi neka kalkulacija. Uredjaja koji izvrsava kvante kalkulacije zovu se \textbf{kvantni racunari}.
Kvantni racunari se sastoje od kvantnih kola i elemntarnih kvantnih kapija koje sluze za prenosenje i manipulisanje kvantnih inforamcija.
\cite{nielsen_chuang_10th} \\
Jedna od glavnih primena Kvantnih racunara jeste simulacija fizickih sistema, bili oni kvantne ili klasince prirode.
%% Pogledati Quantum Computation and Quantum Information za dopune
\subsection{Kvantna inforamcija}
Kvantan informacija je informacija o stanju kvantnog sistema. O njihovim svojstvima bavi se \textbf{kvantna teorija informacije}.
Takodje, kvantan informacija mogu izmeriti na isti nacin kao i klasicna informacija koristeci se Šenononvoj metodi. 
Odnosno, postoji jednistveno merilo to jest funkcija nad kvantnim stanjem, koje je funkcija verovatnoce, kontinuiteta i sumiranja.\cite{vlatko_v}
Ova funkcija se zove \textbf{von Neumann entropy} i za neki ulazni kubit $\varrho$ postoji ekvivalent u  \textbf{Shannon entropy} $H$
za neku slucajnu promenljivu $X$
\[
    S(\varrho) = H(X)
\]
%% Opisati metod u Apendexu mozda
Jos jedna od merila za kvantno stanje jeste merenje "istinitosti" (eng. Fidelity) izmedju dva kvantna stanja $ \ket{\phi}$ i $\ket{\psi}$.
Neka je $F$ funkcija koja meri osobinu, ona meri verovatnocu da merenjem stanja $\ket{\phi}$ dobijemo stanje $\ket{\psi}$.
Izlaz funkcije je izmedju 0 i 1, gde ako je izlaz 0 onda su dva stanja ortogonalna jedna od drugog, a ako je izlaz 1 onda su dva stanja jednaka.\cite{vlatko_v}
\subsubsection*{Odnost kvantne i klasicne teorije informacije}
Kvantna i klasicna informacija se u dosta stvari razlikuju. Dok se klsaicna informacija prolazi kroz sisteme sa dobro definisanim stanjima, moze se kopirati i pri procesom merenje se nemenja,
Kvantna informacija je enkodovana u kvantnim sistemima, ne moze se kopirati i pri procesu merenje ona se menja. Takodje kvantan informacija ima neke osobine koje se ne
mogu iskazati u klasicnoj informaciji, kao sto su superpozicija i kvantan uvezanost \cite{Classical&quantum_info} \\
Kvantana teorija informacije se bavi: 
\begin{enumerate}
    \item Prenosenju klasicnih informacija preko kvantnih kanala
    \item Prenosenje kvantinih informacija preko kvantinh kanala
    \item Efekat kvantne uvezanosti na prenosenje informacija
\end{enumerate}
\subsubsection{Priprema podataka}
Za obradu podataka treba nam kvantni RAM (QRAM), koji nam dozvoljava paralelan pristup kvantnim podacima. 
Neka imamo kopleksan vektor $\overrightarrow{v}$ sa $N=2^n$ dimenzija, gde su njegove komponente oblika 
\[
    v_j = |v'_j|e^{i\varPhi_j}
\]

Ako imamo parove ${|v'_j|,\varPhi_j}$ cuvamo kao float brojeve u QRAM-u, onda mozemo da konstruisemo
$\log_{2}N$ kubit kvantno stanje $\ket{v} = |\overrightarrow{v}|^{-\frac{1}{2}}\overrightarrow{v}$ u $O(\log N)$ koraka

Kada smo kreirali kompresovane kvantne vektore od ulaznih vektora, mozemo da vrsimo transformacije koristeci kvantne algoritme, za dalje koriscenje podataka za masinsko ucenje.
Ovaj proces zove se \textbf{postprocessing} i u opstem obliku njemu je potrebno $O(poly(\log{}N))$ koraka. \cite{lloyd2013quantum}


\section{Linearne algebra za kvantno masinsko ucenje}
Da bi videli kako kvantni racunari poboljsavaju masinsko ucenje, treba da se vidi kako kvantni racunari obradjuju linearnu algebru, jednu od osnova modernog masinskog ucenja. \\
Tokom godina razvijeni su nekoliko kvantnih algoritama koji resavaju probleme linearne algebre.
Zajedno ti algoritmi se nazivaju \textbf{osnovni kvantni podprogrami linearne algebre} (eng. qBLAS), i oni se koriste u izradi algoritama za kvantno masinsko ucenje.

Primeri algoritama koji su deo qBLAS-a su: 
\begin{itemize}
    \item HHL algoritam: koristi se za resavanje sistema linearnih jednacina, koristeci $2^n$ dimenzijonalni vektorski prostor
        za resavanje sistema sa $n$ promenljivih. \cite{Quantum_machine_learning}
    \item Kvantna Furijeova transformacije \cite{Classical&quantum_info_Fourie_Phase}
    \item Kvantan procena faza za eigen vrednosti i eigen vektora/stanja. \cite{Classical&quantum_info_Fourie_Phase}
\end{itemize}
Ovi algoritmi se koriste kao osnova napredinij algoritama i algoritama za Kvantno masinsko ucenje.
Samo treba pripaziti kod pominjanja ovih algoritama, jer neki od njih koriste neke koncepte koji su samo
teorijske prirode ili su tesko kreirani u realnom svetu (npr. QRAM).
\section{Kvantna teorija kompleksnosti}
\section{Kvantno masinsko ucenje}
Kvantno masinsko ucenje je spoj kvantnih racunara i masinskog ucenja. U programima Kvantnog masinskog ucenja koriste se kvantni algoritmi (npr. qBLAS algoritmi)
kao deo metoda optimizacija slicne klasicnim metodama masinskog ucenja. 

Prema vrsti podataka koji se obradjuju oblast mozemo da dalimo na dve podoblasti
\begin{enumerate}
    \item Obrada klasicnih podataka na kvantnim masinama (\textbf{Masinsko ucenje dopunjeno kvantnim racunarima} eng. Quantum-enhanced machine learning)
    \item Obrada kvantnim podataka na kvantnim masinama
\end{enumerate}
Problem kod obrade klasicnih podataka na kvantnim masinama jeste ucitavanje podataka u sistem, kao i citanje rezultata. Ovo dovodi da algoritnim sa teorijskim eksponencijalnim
ubrzanjem, u realnom svetu budu dosta sporiji i fizcki zahtevniji (velicina kvatnog kola zna da poraste i na skalu oko $10^{25}$ za jednostavnu implementaciju HHL algoritma). \cite{Quantum_machine_learning}

\subsection{Quantum support vector machine}
Jedan od nejednostavnijih primera metoda Kvantnog masinskog ucenja jeste \textbf{Quantum support vector machine} (QSVM). Klasican SVM je metoda koja pronalazi optimalnu podelu hiper-ravni
izmedju dva razlicita skupa podataka, tako da sa velikom verovatnocom svi podaci iz jednog skupa podataka ce se naci na jednoj polovini hiper-ravni. \cite{Quantum_machine_learning}
\subsubsection{Klasican algoritam}
Ova metoda odredjuje klase koristeci linarnu funkciju $w^{T}x + b$. SVD predvidja prvu klasu ako je izlaz funkcije je pozitivan, a predvidja drugu klasu je izlaz negativan.
Posto kod vecina slucajeva odvojenost izmedju dve klase podataka nije linearzibilno odvojivo, sa SVM metodom koristi se i \textbf{Kernel metoda}. \\
Pronalazenje optimalne hiper-ravni se sastoji od minimizacije $|w|^{2}/2$ u nejednacini $y_j(w*x_j+b) \geq 1$ za svako j. 
Ovo minimizicija se moze uraditi, ako uvedemo Karush-Kuhn-Tucker mnozioca $\overrightarrow{\alpha} = (\alpha_1,...,\alpha_M)$ i maksimizujemo ih nad Lagranzovoj funkcijom:
\[
    L(\overrightarrow{\alpha}) = \sum_{j=1}^{M}{y_j\alpha_j} - \frac{1}{2}\sum_{j,k=1}^{M}{\alpha_j\alpha_kx_jx_k}
\]
Sa sledecim ogranicenjima $\sum_{j=1}^{M}{\alpha_j=0}$ i $\forall j \leq M $ $y_j\alpha_j \geq 0$. Tako da, parametre za hiper-ravan se izvode kao:
$w = \sum_{j=1}^{M}\alpha_jx_j$ i $b = y_j - wx_j$ (za one $j$ gde vazi da $\alpha_j \neq 0$). Mali broj $\alpha_j$ je razlicitno od nule, takve promenljive se odnose na vectore $x_j$ koji leze na ravni,
ti vektori se zovu \textbf{Support vektori} \cite{rebentrost2014quantum}

Kernel metoda transformise podatke u prostor gde su dve klase linearno odvojive. Metoda se oslanja na to da se linearna funkcija
moze zapisati iskljucivo kao dot prodakt izmedju primera.
\[
    w^{T}x + b = b + \sum_{i=1}^m \alpha_ix^Tx_i
\]
Gde je $x_i$ trening primer a $\alpha$ je vektor koeficijenata. Ovako zapisivanje funkcije nam dozvoljva da zamenim $x$ sa izlazom funkcije $\phi(x)$, a dot prodakt sa funkcijom $k(x,x_i) = \phi(x)*\phi(x_i)$.
Funkcija $k$ se zove \textbf{kernel}, dok funkcija $\psi$ je funkcija koja preslikava podatke iz jednog prostora u drugi. Operator $\langle * \rangle$ predstavlja unutrasnji prodakt ekvivalentno $\phi(x)^T\phi(x_i)$. \cite{goodfellow2016deep}

Kada zamenimo dot prodakt sa kernelom, funkciju predikcije mozemo da zapisemo kao
\[
    f(x) = b + \sum_i \alpha_{i}k(x,x_i)
\]
Jedan od velikih mana kernel metode jeste cena evaluacije izlaza kernel funkcije je linarna u odnosu na broj trening primera, jer $i$-ti bi oznacavao clana $\alpha_ik(x,x_i)$ kernel funkcije. \cite{goodfellow2016deep} \\
Slozenost SVM je $O(log(1/\epsilon)M^2(N+M))$, gde je $\epsilon$ preciznost resenja, $N$ je broj dimenzija prostora nad kojem radimo ,a $M$ je broj trening primera. \\
Takodje krajnje resenje se je binarni klasifikator za neki vektor $x$:
\[
    y(x) = sign(\sum_{j=1}^{M}\alpha_jkk(x,x_j) + b)
\]

\subsubsection{Kvantni algoritam}
Pretpostavimo da imamo metodu za treniranje(eng. Oracle) koja vraca norme $|x_j|$, labele $y_j$ 
i kvanten vektore $\Ket{x_j} = \frac{1}{|x_j|}\sum_{k=1}^{N}(x_j)_k\Ket{k}$. \\
Bitno nam je za algoritam da ova metoda vraca podatke pod donjom granicom, da bi se kompleksost jezgra algoritma mogla iskazati.
Koristeci evaluaciju inner prodakt priprema se kernel matrica, moze se dobiti SVD algoritam kompleksnoscu $O(log(1/\epsilon)M^3 + M^{2}\log(N/\epsilon))$
Kernel matrica je od velike vaznosti za reformulaciju algoritma kao funkciju kvadratnog troska. 
Uvodimo simplifikaciju za nejednakosti, tako sto uvocimo promenljivu $e_j$ i koristimo osoboinu labela da $y_j^2=1$
\[
    y_{j}(w \cdot x_j + b) \geq 1 \to (w \cdot x_j + b) = y_{j} - y_{j}e_{j}
\]
Porod ove jednacine imamo i implicitan uslov Lagranzove funkcije da sadrzi taksanu (eng. penalty) promenljvi
$\gamma/2\sum_{j=1}^{M}e_j^2$ gde definisana $\gamma$ za relativne tezinu greske treniranja.
Ako uzmemo parcijalno derivat od Lagranzove funkcije i eliminisemo promenljivu $u$ i $e_j$ dovodi do aprokcismaciju funkcije kvadratnog troska problema:
\[
    F\begin{bmatrix}
        b \\
        \overrightarrow{\alpha}
    \end{bmatrix}
    \equiv \begin{bmatrix}
        0 & \overrightarrow{1}^T \\
        \overrightarrow{1} & K+\gamma^{-1} \mathbb{1}
    \end{bmatrix}
    \begin{bmatrix}
        b \\
        \overrightarrow{\alpha}
    \end{bmatrix} =
    \begin{bmatrix}
        0 \\
        \overrightarrow{y}
    \end{bmatrix}
\]
Ovde $K_{ij} = x_i^T \cdot x_j$ je simetricna kernel matrica, $y = (y_1,...,y_m)$ kao i
$\overrightarrow{1} = (1,...,1)$. Matrica $F$ je dimenzija $(M+1)\times(M+1)$. Dodatna dimenzija (red i kolona) se sastoji od jedinica,
zbog offset-a $b$. Promenljiva $\alpha_j$ ima ulogu odredjivanje distance od optimalnog resenja. Tako da na resenje, odnonsno
pronalazenje promenljivih za SVM je oblika:
\[
    \begin{bmatrix}
        b \\
        \overrightarrow{\alpha}
    \end{bmatrix} =
    F^{-1} \begin{bmatrix}
        0 \\
        \overrightarrow{y}
    \end{bmatrix}
\]
U klasicnom algoritmu kopleksnost SVM sa funkcijom kvadratnog troska je $O(M^3)$ 

U kvantnom algoritmu, zadatak je generisanje stanja $\Ket{b,\overrightarrow{\alpha}}$ koja opisuju hiper-ravan i onda klasifikuju stanja
$\Ket{x}$. U algoritmu, resavamo normalizovanu jednacinu $\hat{F}\Ket{b,\overrightarrow{\alpha}} = \Ket{y}$, gde je $\hat{F} = F/trF$ sa 
ogranicenjem $\Vert F \Vert \leq 1$. Klasa ce biti odredjenja kao verovatnoca uspeha pri swap testu izmedju $\Ket{b,\overrightarrow{\alpha}}$ i
$\Ket{x}$. Za efikasnost merenje algoritma, posebno izracunavanja interzne matrice, matrica $\hat{F}$ mora da se razdvoji na jednostavne elemente.
Tako da matrica $\hat{F}$ moze da se razdvoji na sledece elemente $\hat{F} = (J+K+\gamma^{-1}\mathbb{1})/trF$. 
Gde je matrica
\[
    J = \begin{bmatrix}
        0 & \overrightarrow{1}^T \\
        \overrightarrow{1} & 0 \\
    \end{bmatrix}
\]
Takodje, za estimaciju faze pravimo formulaciju Lijevog prodakta \\
$e^{-i\hat{F}\varDelta{t}} = e^{-i\gamma^{-1}\mathbb{1}\varDelta{t}/trF}e^{-iJ\varDelta{t}/trF}e^{-iK\varDelta{t}/trF} + O(\varDelta{t})$

Za njega vazi da ima dve eigen vrednosti oblika $\lambda_{\pm} = \pm \sqrt{M}$,a, istovetno, 
eigen stanja su oblika $\Ket{\lambda_{\pm}} = \frac{1}{\sqrt{2}}(\Ket{0} \pm \frac{1}{\sqrt{M}}\sum_{k=1}^M\Ket{k})$.
Za matricu $\gamma^{-1}\mathbb{1}$ dve eigne vrednosti su $v_1 = 0$ i $v_2 = \gamma^{-1}M$.
Sada mozemo da aproksimiramo fazu za $e^{-i\hat{F}\varDelta{t}}$.

Prvi korak,Stanje $\Ket{y}$ moze da se transformise u eigen state $\Ket{u_j}$ matrice $\hat{F}$, koja ima eigen vrednost $\lambda_j$.
Ono je obilka $\Ket{y} = \sum_{j=1}^{M+1}\braket{u_j|y}\Ket{u_j}$. Ako inicijalizujemo aproksimaciju eigen vrednosti na $\Ket{0}$, i primenimo 
estimaciju faze nad stanjem dobicemo stanje blize pravoj eigen vrednosti:
\[
  \Ket{y}\Ket{0} \to \sum_{j=1}^{M+1}\braket{u_j|y}\Ket{u_k}\Ket{\lambda_j} \to \sum_{j=1}^{M+1}\frac{\braket{u_j|y}}{\lambda_j}\Ket{u_j}
\]
Drugi korak je da invertujemo dobijeno stanje eigen vrednosti, pozivajuci rotaciju stanja. Na kraju dobijamo novo stanje sa trazenim parametrima SVM ($C = b^2 + \sum_{k=1}^M{\alpha_k}^2$)
\[
    \Ket{b,\overrightarrow{\alpha}} = \frac{1}{\sqrt{C}}(b\Ket{0}+ \sum_{k=1}^M{\alpha_k \Ket{k}})
\]

\paragraph*{Klasifikacije}
Sada imamo trenirani model kvantnog SVM-a i zelimo da klasifikujemo stanje $\Ket{x}$. Od stanja $\Ket{b,\overrightarrow{\alpha}}$, koriscenjem metode za treniranje, konstruisemo stanje:
\[
    \Ket{\tilde{u}} = \frac{1}{\sqrt{N_u}}(b\Ket{0}\Ket{0} + \sum_{k=1}^M{\alpha_k |x_k| \Ket{k}\Ket{x_k}})
\]
Gde nam je $N_u=b^2+\sum_{k=1}^M{\alpha_k^2 |x_k|^2}$. Pored ovoga konstruisemo i ulazno stanje $\Ket{\tilde{x}}$:
\[
    \Ket{\tilde{x}} = \frac{1}{\sqrt{N_x}}(\Ket{0}\Ket{0} + \sum_{k=1}^M{|x|\Ket{k}\Ket{x}})
\]
Gde nam je $N_x=M|x|^2 + 1$. Konstruisemo dva nova stanja $\Ket{\psi}$ i $\Ket{\phi}$; $\Ket{\psi}=\frac{1}{\sqrt{2}}(\Ket{0}\Ket{\tilde{u}}+\Ket{1}\Ket{\tilde{x}})$ i
$\Ket{\phi} = \frac{1}{\sqrt{2}}(\Ket{0}-\Ket{1})$. Merenjem swap testa, verovatnoca dobivanja pozitivne vrendosti je $P=|\braket{\psi|\phi}|^2=\frac{1}{2}(1-\braket{\tilde{u}|\tilde{x}})$. 
Ovde unutrasnji produkt, odnosno 
$\braket{\tilde{u}|\tilde{x}} = \frac{1}{\sqrt{N_xN_u}}(b+\sum_{k=1}^M{\alpha_k |x_k||x|\braket{x_k|x}})$, koji se obicno izracunava u $O(1)$ na kvatnom racunaru.
Ako hocemo preciznost $\epsilon$, treba da iteriramo kroz algoritam merenja $O(P(1-P)/\epsilon^2)$ puta. \cite{rebentrost2014quantum}
\subsection{Quantum principal component analysis}
\subsubsection{Klasicni algoritam}
Ova je metoda koja se koristni za smanjivanje dimenzija vektora podataka gde nam je bitno da sacuvamo sto vise informacije o podatku -
labava kompresija (eng. lossy compression). \\
Neka za svaku tacku $x^{(i)} \in \mathbb{C}^n$ zelimo da transformisemo u tacku $c^{(i)} \in \mathbb{C}^l$ gde je $l < n$.
Zelimo da nadjemo funkciju enkodovanja koja za ulaz $x$ vraca $c$, odnosno, $f(x)=c$. Takodje zelimo da nadjemo funkciju dekodovanja $g(f(x)) \approx x$.

Zbog jednostavnosti, uzecemo funkciju mnozenja matrica kao funkciju dekodavanja. Neka je $g(c)=Dc$, gde je
$D \in \mathbb{C}^{n \times l}$ matrica definisana za dekodovanje. Takodje zbog optimalno izracunavanja funkcije enkodovanja, 
PCA uvodi ogranicenje da su kolone medjusobno ortogonalne. Jos jedno ogranicenje koje moze da se uvede u algoritam,
i koji ce dovesti do jedinstvenog resenja, jeste da su sve kolone matrice $D$ u unitarnoj normi.
Jedan od nacina na koji hocemo da nadjemo optimalnu projekciju $c$ za ulaz $x$ jeste da nadjemo najmanju L2 distancu izmedju
ulaza $x$ i dekodovane vrednosti $g(c)$
\[
  c^{*} = \underset{c}{\mathrm{argmin}} \Vert x-g(c) \Vert_2^2
\]
I ova za pronalazenje minimalne distance ce dovesti do optimalnog resenje $c=D^{T}x$\cite{goodfellow2016deep}. Tako da funkcija enkodovanja je oblika:
\[
f(x) = D^Tx  
\]
Takodje, mozemo da uvedemo novu funkciju rekonstrukcije ulaza $x$
\[
  r(x) = g(f(x)) = DD^Tx  
\]
Sada treba da se nadje optimalna matrica $D$. Ovo ce se resiti na isti nacin kao i pronalazenje optimalnog $c$ za ulaz $x$, odnostno kao pronalazenje minimalne L2 distance za ulazne vektore
njihove rekonstrukcije.
\[
    D^{*} =\underset{D}{\mathrm{argmin}} \sqrt{\sum_{i,j}(x^i_j - r(x^i)_j)^2} \text{ gde vazi } D^TD = I_l
\]
Posle procesa izvodjenja \cite{goodfellow2016deep}, jendacina za optimalnu matricu $D$ je oblika:
\[
    D^{*} =\underset{D}{\mathrm{argmin}} Tr(D^TX^TXD) \text{ gde vazi } D^TD = I_l
\] 
Gde nam je $X \in C^{m \times n}$ matrica gde su redovi ulazni vektori $x$. Ova jednacna se moze resiti koristeci eigen dekompoziciju. Gde bi 
se pronasli eigen vektor za $X^TX$ za najvecu eigne vrednost. 
\subsubsection{Kvantni algoritam}
U kvantnom algoritmu bitno nam je da nedjenmo eigen vektore i eigen vrednosti za ulaz. Ovo se dosta olanja na drugi deo metode koji je opisan u Support vector machine sekciju.
Ako izaberemo random vektor $v_j$ iz skupa ulaznih vektora,kreiramo kvanto stanje $\Ket{v_j}$; tada mozemo da kreiramo \textit{density} matricu $\rho = (1/N)\sum_j\Ket{v}\Bra{v}$ gde je $N$ velinica skupa vektora. \cite{Quantum_machine_learning}
Slicno \textbf{qSVM} nad \textit{density} matricom $\rho$ mozemo da apliciramo algoritam esitacije faze stanja. Odnosnto, da primenimo $e^{-i \rho t}$, $t$ puta nad inicijalnim stanjem:
\[
    \Ket{\psi}\Ket{0} \to \sum_i \psi_i\Ket{\chi_i}\Ket{r_i}
\] 
Gde je $\Ket{\chi_i}$ eigen vektor od matrice $\rho$, $r_i$ je esimacija eigen vrednosti, a $\psi_i = \Braket{\chi_i|\psi}$

\newpage
\printbibliography
\end{document}